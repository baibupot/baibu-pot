# See https://www.robotstxt.org/robotstxt.html for documentation on how to use the robots.txt file
#
# To allow all robots to crawl your site, use the following:
User-agent: *
Allow: /

# Yandex specific rules
User-agent: Yandex
Allow: /
Crawl-delay: 1

# To disallow all robots from crawling your site, use the following:
# User-agent: *
# Disallow: /

# To specify a sitemap, use the following:
Sitemap: https://baibupot.com/sitemap.xml
Sitemap: https://baibupot.com/yandex.xml
